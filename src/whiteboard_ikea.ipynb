{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import randint\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\":\"Ubuntu Chromium/76.0.3809.100\"}\n",
    "           #Mozilla/5.0 (X11; Linux x86_64) \n",
    "           #AppleWebKit/537.36 (KHTML, like Gecko) \n",
    "           #Chrome/76.0.3809.100 Safari/537.36}\n",
    "ikea_base = 'https://www.ikea.com'\n",
    "\n",
    "# find the link to every family\n",
    "all_families = []\n",
    "for ii in range(0, 25):  # every letter in the alphabet\n",
    "    url = ikea_base + '/us/en/catalog/productsaz/' + str(ii) + '/'\n",
    "    r = requests.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    # the container for all product family links\n",
    "    # some letters do not have product families -- use exception handling\n",
    "    product_list = soup.find_all(\"li\",{\"class\":\"productsAzLink\"})\n",
    "\n",
    "    # extract link to each family-s splash page\n",
    "    # if there aren't any products on that page, skip to next iteration\n",
    "    if(len(product_list) == 0):\n",
    "        continue\n",
    "    else:\n",
    "        family_links = [] \n",
    "        for req in product_list:\n",
    "            family_hrefs = req.find_all(\"a\", href=True)\n",
    "            for link_element in family_hrefs:\n",
    "                family_links.append(link_element['href'])\n",
    "        all_families.append(family_links)\n",
    "\n",
    "    \n",
    "# all_families is a list of lists\n",
    "# list elements are each a list of family links\n",
    "# flatten this list\n",
    "all_families = [val for sublist in all_families for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each family get all products\n",
    "all_products = []\n",
    "for fl in all_families:\n",
    "    url = ikea_base + fl\n",
    "    r = requests.get(url, headers = headers)\n",
    "    family_soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    item_list = family_soup.find_all(\"div\",{\"class\":\"productLists\"})\n",
    "    \n",
    "    item_links = []  # links to each product\n",
    "    for req in item_list:\n",
    "        item_hrefs = req.find_all(\"a\", {\"class\":\"productLink\"}, href=True)\n",
    "        for link_element in item_hrefs:\n",
    "            item_links.append(link_element['href'])\n",
    "    \n",
    "    all_products.append(item_links)\n",
    "\n",
    "    \n",
    "# family_item_links is a list of lists\n",
    "# list elements are each a list of product links\n",
    "# flatten this list\n",
    "all_products = [val for sublist in all_products for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_picture = []\n",
    "info_corpus = []\n",
    "for pp in all_products:\n",
    "    url = ikea_base + pp\n",
    "    r = requests.get(url, headers = headers)\n",
    "    product_soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    # image of product\n",
    "    product_img = product_soup.find(\"div\", {\"class\":\"rightContentContainer\"}).img\n",
    "    info_picture.append(product_img['src'])\n",
    "    \n",
    "    # text description information\n",
    "    product_info = product_soup.find_all('div', {\"id\":\"custBenefit\"}, {\"class\":\"texts keyFeaturesmargin\"})\n",
    "\n",
    "    info_list = []\n",
    "    for req in product_info:\n",
    "        list_item = req.find_all('div')\n",
    "        for li in list_item:\n",
    "            info_list.append(li.get_text())\n",
    "    \n",
    "    info_list = [i.replace('-', '') for i in info_list]\n",
    "    info_corpus.append(' '.join(info_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(list(zip(info_picture, info_corpus)),\n",
    "                      columns = ['img_url', 'description'])\n",
    "\n",
    "output.to_csv('/home/peter/Documents/projects/insight/copyprisim/results/example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
