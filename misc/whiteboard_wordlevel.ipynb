{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Z-18-cGIjQEx",
    "outputId": "5ed96a17-7f9d-48e0-e381-bae50d24495f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AnLEY-TYb6v"
   },
   "outputs": [],
   "source": [
    "!cp drive/My\\ Drive/ikea.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6OW_nlk-3ML"
   },
   "outputs": [],
   "source": [
    "!cp drive/My\\ Drive/ikea_2.csv .\n",
    "# !cp ikea_2.csv drive/My\\ Drive/ikea_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMFSR-_Ei4zb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randint\n",
    "from pickle import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "random.seed(952)\n",
    "\n",
    "\n",
    "def clean_text(input):\n",
    "  # tokenizer\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  tokens = tokenizer.tokenize(input)\n",
    "  \n",
    "  # remove punctuation\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "  # remove non alphabetic \n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  \n",
    "\t# make lower case\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "\n",
    "# save tokens to file, one sequence per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "  \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iU_Lb2hY6wvD"
   },
   "outputs": [],
   "source": [
    "ikea_items = pd.read_csv('ikea_2.csv')\n",
    "\n",
    "# some items do not have descriptions from the specific box\n",
    "ikea_items = ikea_items.dropna()\n",
    "\n",
    "# some descriptions are identical\n",
    "desc_uni = ikea_items.drop_duplicates(subset='description')\n",
    "\n",
    "# average description length for future generation\n",
    "# desc_avg = round(sum( map(len, desc_uni) ) / len(desc_uni))\n",
    "# desc_std = map(len, desc_uni).std()\n",
    "\n",
    "# split train and test\n",
    "desc_train, desc_test = train_test_split(desc_uni, test_size=0.2)\n",
    "pd.DataFrame(desc_train).to_csv('ikea_word_train.csv')\n",
    "pd.DataFrame(desc_test).to_csv('ikea_word_test.csv')\n",
    "\n",
    "# make one corpus\n",
    "desc_single = ' '.join(desc_train.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Jn8A-zrL5oO3",
    "outputId": "bd00bbb3-4701-4ac6-e002-0684b60cdf6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 62705\n",
      "Unique Tokens: 2860\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_text(desc_single)\n",
    "\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# make sequences of words from the full corpus\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "  \n",
    "#print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "out_filename = 'ikea_word_train_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JX8TcM69j_ru"
   },
   "outputs": [],
   "source": [
    "!cp ikea_word_test.csv drive/My\\ Drive/.\n",
    "!cp ikea_word_train.csv drive/My\\ Drive/.\n",
    "\n",
    "!cp ikea_word_train_sequences.txt drive/My\\ Drive/.\n",
    "\n",
    "# !cp drive/My\\ Drive/ikea_word_train_sequences.txt ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6dkfMiXZ1l3a",
    "outputId": "1a9d1138-3d7d-496c-b4b1-e2f821b72fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sequences: 62654\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "in_filename = 'ikea_word_train_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "print('total sequences: %d' % len(lines))\n",
    "# code as integers\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "dump(tokenizer, open('word_tokenizer.pkl', 'wb'))\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "model.add(LSTM(100))#, return_sequences=True))\n",
    "# You must set return_sequences=True when stacking LSTM layers so that the \n",
    "# second LSTM layer has a three-dimensional sequence input\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "# print(model.summary())\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# helpful checkpoints\n",
    "filepath = \"word_model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss',  # 'accuracy'\n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VAPmQFpQ3Ond",
    "outputId": "4de67306-c866-4512-928c-245e1e044b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62654/62654 [==============================] - 27s 438us/step - loss: 6.2949 - acc: 0.0677\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.29491, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 2/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 5.8835 - acc: 0.0684\n",
      "\n",
      "Epoch 00002: loss improved from 6.29491 to 5.88350, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 3/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 5.5955 - acc: 0.0997\n",
      "\n",
      "Epoch 00003: loss improved from 5.88350 to 5.59549, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 4/100\n",
      "62654/62654 [==============================] - 27s 423us/step - loss: 5.2462 - acc: 0.1526\n",
      "\n",
      "Epoch 00004: loss improved from 5.59549 to 5.24618, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 5/100\n",
      "62654/62654 [==============================] - 26s 418us/step - loss: 4.9347 - acc: 0.1958\n",
      "\n",
      "Epoch 00005: loss improved from 5.24618 to 4.93466, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 6/100\n",
      "62654/62654 [==============================] - 26s 423us/step - loss: 4.6789 - acc: 0.2255\n",
      "\n",
      "Epoch 00006: loss improved from 4.93466 to 4.67890, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 7/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 4.4581 - acc: 0.2471\n",
      "\n",
      "Epoch 00007: loss improved from 4.67890 to 4.45807, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 8/100\n",
      "62654/62654 [==============================] - 26s 423us/step - loss: 4.2455 - acc: 0.2680\n",
      "\n",
      "Epoch 00008: loss improved from 4.45807 to 4.24551, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 9/100\n",
      "62654/62654 [==============================] - 27s 428us/step - loss: 4.0366 - acc: 0.2900\n",
      "\n",
      "Epoch 00009: loss improved from 4.24551 to 4.03662, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 10/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 3.8412 - acc: 0.3139\n",
      "\n",
      "Epoch 00010: loss improved from 4.03662 to 3.84118, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 11/100\n",
      "62654/62654 [==============================] - 26s 418us/step - loss: 3.6606 - acc: 0.3341\n",
      "\n",
      "Epoch 00011: loss improved from 3.84118 to 3.66065, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 12/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 3.4951 - acc: 0.3553\n",
      "\n",
      "Epoch 00012: loss improved from 3.66065 to 3.49514, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 13/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 3.3423 - acc: 0.3771\n",
      "\n",
      "Epoch 00013: loss improved from 3.49514 to 3.34232, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 14/100\n",
      "62654/62654 [==============================] - 26s 421us/step - loss: 3.2028 - acc: 0.3951\n",
      "\n",
      "Epoch 00014: loss improved from 3.34232 to 3.20278, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 15/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 3.0743 - acc: 0.4126\n",
      "\n",
      "Epoch 00015: loss improved from 3.20278 to 3.07427, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 16/100\n",
      "62654/62654 [==============================] - 27s 430us/step - loss: 2.9531 - acc: 0.4305\n",
      "\n",
      "Epoch 00016: loss improved from 3.07427 to 2.95308, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 17/100\n",
      "62654/62654 [==============================] - 27s 432us/step - loss: 2.8419 - acc: 0.4461\n",
      "\n",
      "Epoch 00017: loss improved from 2.95308 to 2.84189, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 18/100\n",
      "62654/62654 [==============================] - 27s 429us/step - loss: 2.7361 - acc: 0.4614\n",
      "\n",
      "Epoch 00018: loss improved from 2.84189 to 2.73613, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 19/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 2.6378 - acc: 0.4789\n",
      "\n",
      "Epoch 00019: loss improved from 2.73613 to 2.63782, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 20/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 2.5459 - acc: 0.4940\n",
      "\n",
      "Epoch 00020: loss improved from 2.63782 to 2.54592, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 21/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 2.4570 - acc: 0.5092\n",
      "\n",
      "Epoch 00021: loss improved from 2.54592 to 2.45699, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 22/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 2.3744 - acc: 0.5253\n",
      "\n",
      "Epoch 00022: loss improved from 2.45699 to 2.37436, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 23/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 2.2962 - acc: 0.5385\n",
      "\n",
      "Epoch 00023: loss improved from 2.37436 to 2.29622, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 24/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 2.2227 - acc: 0.5531\n",
      "\n",
      "Epoch 00024: loss improved from 2.29622 to 2.22274, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 25/100\n",
      "62654/62654 [==============================] - 27s 423us/step - loss: 2.1531 - acc: 0.5642\n",
      "\n",
      "Epoch 00025: loss improved from 2.22274 to 2.15311, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 26/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 2.0862 - acc: 0.5777\n",
      "\n",
      "Epoch 00026: loss improved from 2.15311 to 2.08618, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 27/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 2.0238 - acc: 0.5895\n",
      "\n",
      "Epoch 00027: loss improved from 2.08618 to 2.02381, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 28/100\n",
      "62654/62654 [==============================] - 27s 429us/step - loss: 1.9635 - acc: 0.6012\n",
      "\n",
      "Epoch 00028: loss improved from 2.02381 to 1.96349, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 29/100\n",
      "62654/62654 [==============================] - 27s 427us/step - loss: 1.9079 - acc: 0.6112\n",
      "\n",
      "Epoch 00029: loss improved from 1.96349 to 1.90790, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 30/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 1.8536 - acc: 0.6206\n",
      "\n",
      "Epoch 00030: loss improved from 1.90790 to 1.85358, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 31/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 1.8032 - acc: 0.6292\n",
      "\n",
      "Epoch 00031: loss improved from 1.85358 to 1.80319, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 32/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 1.7544 - acc: 0.6406\n",
      "\n",
      "Epoch 00032: loss improved from 1.80319 to 1.75445, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 33/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 1.7066 - acc: 0.6492\n",
      "\n",
      "Epoch 00033: loss improved from 1.75445 to 1.70658, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 34/100\n",
      "62654/62654 [==============================] - 27s 423us/step - loss: 1.6643 - acc: 0.6568\n",
      "\n",
      "Epoch 00034: loss improved from 1.70658 to 1.66427, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 35/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 1.6207 - acc: 0.6643\n",
      "\n",
      "Epoch 00035: loss improved from 1.66427 to 1.62073, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 36/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 1.5807 - acc: 0.6724\n",
      "\n",
      "Epoch 00036: loss improved from 1.62073 to 1.58074, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 37/100\n",
      "62654/62654 [==============================] - 26s 423us/step - loss: 1.5419 - acc: 0.6797\n",
      "\n",
      "Epoch 00037: loss improved from 1.58074 to 1.54187, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 38/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 1.5058 - acc: 0.6868\n",
      "\n",
      "Epoch 00038: loss improved from 1.54187 to 1.50585, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 39/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 1.4697 - acc: 0.6932\n",
      "\n",
      "Epoch 00039: loss improved from 1.50585 to 1.46968, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 40/100\n",
      "62654/62654 [==============================] - 26s 423us/step - loss: 1.4359 - acc: 0.7006\n",
      "\n",
      "Epoch 00040: loss improved from 1.46968 to 1.43594, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 41/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 1.4124 - acc: 0.7057\n",
      "\n",
      "Epoch 00041: loss improved from 1.43594 to 1.41237, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 42/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 1.3737 - acc: 0.7121\n",
      "\n",
      "Epoch 00042: loss improved from 1.41237 to 1.37375, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 43/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 1.3438 - acc: 0.7170\n",
      "\n",
      "Epoch 00043: loss improved from 1.37375 to 1.34378, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 44/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 1.3139 - acc: 0.7237\n",
      "\n",
      "Epoch 00044: loss improved from 1.34378 to 1.31391, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 45/100\n",
      "62654/62654 [==============================] - 26s 421us/step - loss: 1.2851 - acc: 0.7300\n",
      "\n",
      "Epoch 00045: loss improved from 1.31391 to 1.28507, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 46/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 1.2580 - acc: 0.7346\n",
      "\n",
      "Epoch 00046: loss improved from 1.28507 to 1.25799, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 47/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 1.2323 - acc: 0.7415\n",
      "\n",
      "Epoch 00047: loss improved from 1.25799 to 1.23232, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 48/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 1.2073 - acc: 0.7469\n",
      "\n",
      "Epoch 00048: loss improved from 1.23232 to 1.20729, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 49/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 1.1821 - acc: 0.7512\n",
      "\n",
      "Epoch 00049: loss improved from 1.20729 to 1.18206, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 50/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 1.1583 - acc: 0.7563\n",
      "\n",
      "Epoch 00050: loss improved from 1.18206 to 1.15826, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 51/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 1.1361 - acc: 0.7611\n",
      "\n",
      "Epoch 00051: loss improved from 1.15826 to 1.13605, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 52/100\n",
      "62654/62654 [==============================] - 27s 423us/step - loss: 1.1128 - acc: 0.7645\n",
      "\n",
      "Epoch 00052: loss improved from 1.13605 to 1.11281, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 53/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 1.0922 - acc: 0.7689\n",
      "\n",
      "Epoch 00053: loss improved from 1.11281 to 1.09222, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 54/100\n",
      "62654/62654 [==============================] - 27s 423us/step - loss: 1.0703 - acc: 0.7731\n",
      "\n",
      "Epoch 00054: loss improved from 1.09222 to 1.07027, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 55/100\n",
      "62654/62654 [==============================] - 27s 427us/step - loss: 1.0495 - acc: 0.7782\n",
      "\n",
      "Epoch 00055: loss improved from 1.07027 to 1.04950, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 56/100\n",
      "62654/62654 [==============================] - 27s 427us/step - loss: 1.0296 - acc: 0.7817\n",
      "\n",
      "Epoch 00056: loss improved from 1.04950 to 1.02964, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 57/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 1.0108 - acc: 0.7863\n",
      "\n",
      "Epoch 00057: loss improved from 1.02964 to 1.01076, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 58/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 0.9914 - acc: 0.7894\n",
      "\n",
      "Epoch 00058: loss improved from 1.01076 to 0.99137, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 59/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 0.9743 - acc: 0.7937\n",
      "\n",
      "Epoch 00059: loss improved from 0.99137 to 0.97429, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 60/100\n",
      "62654/62654 [==============================] - 27s 425us/step - loss: 0.9561 - acc: 0.7963\n",
      "\n",
      "Epoch 00060: loss improved from 0.97429 to 0.95615, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 61/100\n",
      "62654/62654 [==============================] - 27s 427us/step - loss: 0.9377 - acc: 0.8006\n",
      "\n",
      "Epoch 00061: loss improved from 0.95615 to 0.93766, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 62/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 0.9211 - acc: 0.8029\n",
      "\n",
      "Epoch 00062: loss improved from 0.93766 to 0.92108, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 63/100\n",
      "62654/62654 [==============================] - 27s 428us/step - loss: 0.9056 - acc: 0.8066\n",
      "\n",
      "Epoch 00063: loss improved from 0.92108 to 0.90559, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 64/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 0.8889 - acc: 0.8098\n",
      "\n",
      "Epoch 00064: loss improved from 0.90559 to 0.88888, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 65/100\n",
      "62654/62654 [==============================] - 26s 418us/step - loss: 0.8735 - acc: 0.8140\n",
      "\n",
      "Epoch 00065: loss improved from 0.88888 to 0.87352, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 66/100\n",
      "62654/62654 [==============================] - 26s 417us/step - loss: 0.8578 - acc: 0.8164\n",
      "\n",
      "Epoch 00066: loss improved from 0.87352 to 0.85782, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 67/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 0.8418 - acc: 0.8198\n",
      "\n",
      "Epoch 00067: loss improved from 0.85782 to 0.84183, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 68/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 0.8284 - acc: 0.8227\n",
      "\n",
      "Epoch 00068: loss improved from 0.84183 to 0.82841, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 69/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.8134 - acc: 0.8262\n",
      "\n",
      "Epoch 00069: loss improved from 0.82841 to 0.81345, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 70/100\n",
      "62654/62654 [==============================] - 26s 418us/step - loss: 0.8003 - acc: 0.8280\n",
      "\n",
      "Epoch 00070: loss improved from 0.81345 to 0.80028, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 71/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 0.7868 - acc: 0.8314\n",
      "\n",
      "Epoch 00071: loss improved from 0.80028 to 0.78679, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 72/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.7739 - acc: 0.8335\n",
      "\n",
      "Epoch 00072: loss improved from 0.78679 to 0.77389, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 73/100\n",
      "62654/62654 [==============================] - 27s 426us/step - loss: 0.7604 - acc: 0.8369\n",
      "\n",
      "Epoch 00073: loss improved from 0.77389 to 0.76041, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 74/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 0.7476 - acc: 0.8394\n",
      "\n",
      "Epoch 00074: loss improved from 0.76041 to 0.74764, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 75/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 0.7370 - acc: 0.8419\n",
      "\n",
      "Epoch 00075: loss improved from 0.74764 to 0.73698, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 76/100\n",
      "62654/62654 [==============================] - 26s 421us/step - loss: 0.7245 - acc: 0.8439\n",
      "\n",
      "Epoch 00076: loss improved from 0.73698 to 0.72449, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 77/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 0.7116 - acc: 0.8473\n",
      "\n",
      "Epoch 00077: loss improved from 0.72449 to 0.71164, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 78/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 0.7010 - acc: 0.8497\n",
      "\n",
      "Epoch 00078: loss improved from 0.71164 to 0.70099, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 79/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.6896 - acc: 0.8515\n",
      "\n",
      "Epoch 00079: loss improved from 0.70099 to 0.68963, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 80/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 0.6784 - acc: 0.8547\n",
      "\n",
      "Epoch 00080: loss improved from 0.68963 to 0.67843, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 81/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 0.6670 - acc: 0.8577\n",
      "\n",
      "Epoch 00081: loss improved from 0.67843 to 0.66702, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 82/100\n",
      "62654/62654 [==============================] - 27s 424us/step - loss: 0.6570 - acc: 0.8582\n",
      "\n",
      "Epoch 00082: loss improved from 0.66702 to 0.65699, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 83/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.6462 - acc: 0.8619\n",
      "\n",
      "Epoch 00083: loss improved from 0.65699 to 0.64620, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 84/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.6360 - acc: 0.8641\n",
      "\n",
      "Epoch 00084: loss improved from 0.64620 to 0.63597, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 85/100\n",
      "62654/62654 [==============================] - 26s 417us/step - loss: 0.6260 - acc: 0.8660\n",
      "\n",
      "Epoch 00085: loss improved from 0.63597 to 0.62597, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 86/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 0.6162 - acc: 0.8683\n",
      "\n",
      "Epoch 00086: loss improved from 0.62597 to 0.61619, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 87/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 0.6109 - acc: 0.8687\n",
      "\n",
      "Epoch 00087: loss improved from 0.61619 to 0.61089, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 88/100\n",
      "62654/62654 [==============================] - 26s 418us/step - loss: 0.5970 - acc: 0.8727\n",
      "\n",
      "Epoch 00088: loss improved from 0.61089 to 0.59703, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 89/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.5871 - acc: 0.8741\n",
      "\n",
      "Epoch 00089: loss improved from 0.59703 to 0.58705, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 90/100\n",
      "62654/62654 [==============================] - 26s 421us/step - loss: 0.5807 - acc: 0.8760\n",
      "\n",
      "Epoch 00090: loss improved from 0.58705 to 0.58074, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 91/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.5711 - acc: 0.8775\n",
      "\n",
      "Epoch 00091: loss improved from 0.58074 to 0.57114, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 92/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.5605 - acc: 0.8800\n",
      "\n",
      "Epoch 00092: loss improved from 0.57114 to 0.56048, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 93/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 0.5531 - acc: 0.8815\n",
      "\n",
      "Epoch 00093: loss improved from 0.56048 to 0.55307, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 94/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 0.5439 - acc: 0.8843\n",
      "\n",
      "Epoch 00094: loss improved from 0.55307 to 0.54389, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 95/100\n",
      "62654/62654 [==============================] - 26s 419us/step - loss: 0.5355 - acc: 0.8859\n",
      "\n",
      "Epoch 00095: loss improved from 0.54389 to 0.53547, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 96/100\n",
      "62654/62654 [==============================] - 26s 421us/step - loss: 0.5274 - acc: 0.8877\n",
      "\n",
      "Epoch 00096: loss improved from 0.53547 to 0.52737, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 97/100\n",
      "62654/62654 [==============================] - 26s 420us/step - loss: 0.5194 - acc: 0.8899\n",
      "\n",
      "Epoch 00097: loss improved from 0.52737 to 0.51943, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 98/100\n",
      "62654/62654 [==============================] - 27s 423us/step - loss: 0.5100 - acc: 0.8908\n",
      "\n",
      "Epoch 00098: loss improved from 0.51943 to 0.51004, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 99/100\n",
      "62654/62654 [==============================] - 26s 421us/step - loss: 0.5033 - acc: 0.8920\n",
      "\n",
      "Epoch 00099: loss improved from 0.51004 to 0.50332, saving model to word_model_weights_saved.hdf5\n",
      "Epoch 100/100\n",
      "62654/62654 [==============================] - 26s 422us/step - loss: 0.4953 - acc: 0.8935\n",
      "\n",
      "Epoch 00100: loss improved from 0.50332 to 0.49526, saving model to word_model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee104f9438>"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=100, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZ3ISUom3Ou7"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('ikea_word_model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('word_tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFOYPqspIQw_"
   },
   "outputs": [],
   "source": [
    "!cp ikea_word_model.h5 drive/My\\ Drive/.\n",
    "!cp word_tokenizer.pkl drive/My\\ Drive/.\n",
    "!cp word_model_weights_saved.hdf5 drive/My\\ Drive/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "27gZNmjB5wMV",
    "outputId": "4c31360b-a839-4aab-f8e2-dda43850375d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are adjustable so you can customize your storage as needed stationary shelf for high stability adjustable feet for stability on uneven floors the door s integrated dampers allow it to close slowly silently and softly hinges with snap on function are easy to fit without screws built in cable management for\n",
      "\n",
      "collecting cables and cords out of sight but close at hand when you need them smooth running drawers with pull out stop you can easily customize the size of the drawer by moving the divider you can easily see and reach your things because the drawers pull out fully drawers\n",
      "Total Tokens: 15007\n",
      "Unique Tokens: 1812\n",
      "Total Sequences: 14956\n"
     ]
    }
   ],
   "source": [
    "## load cleaned text sequences\n",
    "in_filename = 'ikea_word_train_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('ikea_word_model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('word_tokenizer.pkl', 'rb'))\n",
    "\n",
    "# demo it works on the training dataset\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)\n",
    "\n",
    "# move on to processing the test set into the right shape\n",
    "# i've split train/test by objects, not sequences\n",
    "# make the testing data the right shape to test with\n",
    "ikea_test = pd.read_csv('ikea_word_test.csv')\n",
    "\n",
    "test_desc_single = ' '.join(ikea_test.description)\n",
    "\n",
    "test_tokens = clean_text(test_desc_single)\n",
    "\n",
    "print('Total Tokens: %d' % len(test_tokens))\n",
    "print('Unique Tokens: %d' % len(set(test_tokens)))\n",
    "\n",
    "# make sequences of words from the full corpus\n",
    "length = 50 + 1\n",
    "test_sequences = list()\n",
    "for i in range(length, len(test_tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = test_tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\ttest_sequences.append(line)\n",
    "  \n",
    "print('Total Sequences: %d' % len(test_sequences))\n",
    "\n",
    "out_filename = 'ikea_word_test_sequences.txt'\n",
    "save_doc(test_sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmVtGwGIPWP2"
   },
   "outputs": [],
   "source": [
    "!cp ikea_word_test_sequences.txt drive/My\\ Drive/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q593jQkyPTjN"
   },
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'ikea_word_test_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "test_lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Iwf0UAOi4K9S",
    "outputId": "e0cbd16e-008b-4b7b-97c6-3965d091b6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6974924774322969\n"
     ]
    }
   ],
   "source": [
    "seq_length = len(test_lines[0].split()) - 1\n",
    "\n",
    "out = []\n",
    "for ii in range(0, len(test_lines) - 1):\n",
    "  test_x = ' '.join(test_lines[ii].split()[:-1])\n",
    "  test_y = test_lines[ii].split()[-1]\n",
    "  # print(test_x)\n",
    "  # print(test_y)\n",
    "\n",
    "  res = generate_seq(model, tokenizer, seq_length, test_x, 1)\n",
    "  out.append(res == test_y)\n",
    "\n",
    "  \n",
    "acc_my = sum(out) / len(out)\n",
    "print(acc_my)\n",
    "# X, y = test_sequences[:,:-1], test_sequences[:,-1]\n",
    "\n",
    "dump(acc_my, open('word_test_accuracy.pkl', 'wb'))\n",
    "# generate_seq(model, tokenizer, seq_length, test_lines[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdkbHGa4tp8g"
   },
   "outputs": [],
   "source": [
    "!cp word_test_accuracy.pkl drive/My\\ Drive/."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "whiteboard_wordlevel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
