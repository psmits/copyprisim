{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/peter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randint\n",
    "from pickle import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import textstat\n",
    "import nltk\n",
    "from itertools import compress\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# i'm not even using these anymore\n",
    "nltk.download('stopwords')\n",
    "\n",
    "random.seed(952)\n",
    "\n",
    "\n",
    "def clean_text(input):\n",
    "    # tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    \n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove non alphabetic \n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "  \n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # remove tokens of length 1\n",
    "    tokens_len = [len(i) > 1 for i in tokens]\n",
    "    tokens_filter = list(compress(tokens, tokens_len))\n",
    "    tokens = tokens_filter\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# save tokens to file, one sequence per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        \n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "                \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# generate a follow along sequence from a language model\n",
    "def generate_along(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        \n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "                \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('../results/ikea_word_model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('../results/word_tokenizer.pkl', 'rb'))\n",
    "\n",
    "# load the testing data\n",
    "in_filename = '../results/ikea_word_test_sequences.txt'\n",
    "test_sequences = load_doc(in_filename)\n",
    "test_lines = test_sequences.split('\\n')\n",
    "\n",
    "# move on to processing the test set into the right shape\n",
    "# i've split train/test by objects, not sequences\n",
    "# make the testing data the right shape to test with\n",
    "ikea_test = pd.read_csv('../results/ikea_word_test.csv')\n",
    "\n",
    "test_desc_single = ' '.join(ikea_test.description)\n",
    "\n",
    "test_tokens = clean_text(test_desc_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 15279\n",
      "Unique Tokens: 1718\n",
      "Total Sequences: 15228\n"
     ]
    }
   ],
   "source": [
    "print('Total Tokens: %d' % len(test_tokens))\n",
    "print('Unique Tokens: %d' % len(set(test_tokens)))\n",
    "\n",
    "print('Total Sequences: %d' % len(test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore other similarity metrics\n",
    "ikea_test = pd.read_csv('../results/ikea_word_test.csv')\n",
    "seq_length = len(test_lines[0].split()) - 1\n",
    "\n",
    "test_long = []\n",
    "for item in ikea_test.description:\n",
    "    if len(item.split()) > 50:\n",
    "        test_long.append(item)\n",
    "\n",
    "distances = []\n",
    "for ii in test_long:\n",
    "    exam = clean_text(test_long[1])\n",
    "\n",
    "    opener = exam[:50]\n",
    "    closer = exam[50:]\n",
    "    \n",
    "    # print(' '.join(closer))\n",
    "    \n",
    "    # fill in rest of description\n",
    "    res = generate_seq(model, tokenizer, seq_length, opener, len(closer))\n",
    "    # print(res)\n",
    "    \n",
    "    #\n",
    "    rand_tokens = test_tokens\n",
    "    random.shuffle(rand_tokens)\n",
    "    rand_out = ' '.join(rand_tokens[:len(closer)])\n",
    "    \n",
    "    # to liked format\n",
    "    ref = tokenizer.texts_to_matrix([' '.join(closer)], mode='tfidf')[0]\n",
    "    gen = tokenizer.texts_to_matrix([res], mode='tfidf')[0]\n",
    "    ran = tokenizer.texts_to_matrix([rand_out], mode='tfidf')[0]\n",
    "    \n",
    "    ref_a = ref.reshape(1, len(ref))\n",
    "    gen_a = gen.reshape(1, len(gen))\n",
    "    ran_a = ran.reshape(1, len(ran))\n",
    "    \n",
    "    ref2gen = cosine_similarity(ref_a, gen_a)[0][0]\n",
    "    \n",
    "    ref2ran = cosine_similarity(ref_a, ran_a)[0][0]\n",
    "    \n",
    "    # how much closer is gen to ref than ran is to ref\n",
    "    distances.append(ref2gen - ref2ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_res = pd.DataFrame({'distance': distances})\n",
    "dist_res.to_csv('../results/test_distances.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
