{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/peter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randint\n",
    "from pickle import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import textstat\n",
    "import nltk\n",
    "from itertools import compress\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# i'm not even using these anymore\n",
    "nltk.download('stopwords')\n",
    "\n",
    "random.seed(952)\n",
    "\n",
    "\n",
    "def clean_text(input):\n",
    "    # tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    \n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove non alphabetic \n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "  \n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # remove tokens of length 1\n",
    "    tokens_len = [len(i) > 1 for i in tokens]\n",
    "    tokens_filter = list(compress(tokens, tokens_len))\n",
    "    tokens = tokens_filter\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# save tokens to file, one sequence per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('../results/ikea_word_model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('../results/word_tokenizer.pkl', 'rb'))\n",
    "\n",
    "# load the testing data\n",
    "in_filename = '../results/ikea_word_test_sequences.txt'\n",
    "test_sequences = load_doc(in_filename)\n",
    "test_lines = test_sequences.split('\\n')\n",
    "\n",
    "# move on to processing the test set into the right shape\n",
    "# i've split train/test by objects, not sequences\n",
    "# make the testing data the right shape to test with\n",
    "ikea_test = pd.read_csv('../results/ikea_word_test.csv')\n",
    "\n",
    "test_desc_single = ' '.join(ikea_test.description)\n",
    "\n",
    "test_tokens = clean_text(test_desc_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 15279\n",
      "Unique Tokens: 1718\n",
      "Total Sequences: 15228\n"
     ]
    }
   ],
   "source": [
    "print('Total Tokens: %d' % len(test_tokens))\n",
    "print('Unique Tokens: %d' % len(set(test_tokens)))\n",
    "\n",
    "print('Total Sequences: %d' % len(test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore other similarity metrics\n",
    "ikea_test = pd.read_csv('../results/ikea_word_test.csv')\n",
    "seq_length = len(test_lines[0].split()) - 1\n",
    "\n",
    "test_long = []\n",
    "for item in ikea_test.description:\n",
    "    if len(item.split()) > 50:\n",
    "        test_long.append(item)\n",
    "\n",
    "\n",
    "opens = []\n",
    "refs = []\n",
    "gens = []\n",
    "rans = []\n",
    "distances = []\n",
    "for tl in test_long:\n",
    "    exam = clean_text(tl)\n",
    "\n",
    "    opener = exam[:50]  # seed text\n",
    "    opens.append(' '.join(opener))\n",
    "    \n",
    "    closer = exam[50:]  # reference continue\n",
    "    refs.append(' '.join(closer))\n",
    "    \n",
    "    # print(' '.join(closer))\n",
    "    \n",
    "    #  generated text\n",
    "    res = generate_seq(model, \n",
    "                       tokenizer, \n",
    "                       seq_length, \n",
    "                       ' '.join(opener), \n",
    "                       len(closer))\n",
    "    gens.append(res)\n",
    "    \n",
    "    # random text\n",
    "    rand_tokens = test_tokens\n",
    "    random.shuffle(rand_tokens)\n",
    "    rand_out = ' '.join(rand_tokens[:len(closer)])\n",
    "    rans.append(rand_out)\n",
    "    \n",
    "    # to liked format\n",
    "    ref = tokenizer.texts_to_matrix([' '.join(closer)], mode='tfidf')[0]\n",
    "    gen = tokenizer.texts_to_matrix([res], mode='tfidf')[0]\n",
    "    ran = tokenizer.texts_to_matrix([rand_out], mode='tfidf')[0]\n",
    "    \n",
    "    ref_a = ref.reshape(1, len(ref))\n",
    "    gen_a = gen.reshape(1, len(gen))\n",
    "    ran_a = ran.reshape(1, len(ran))\n",
    "    \n",
    "    ref2gen = cosine_similarity(ref_a, gen_a)[0][0]\n",
    "    \n",
    "    ref2ran = cosine_similarity(ref_a, ran_a)[0][0]\n",
    "    \n",
    "    # how much closer is gen to ref than ran is to ref\n",
    "    distances.append(ref2gen - ref2ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(list(zip(opens, refs, gens, rans, distances)), \n",
    "                      columns = ['start', \n",
    "                                 'reference', \n",
    "                                 'generated', \n",
    "                                 'random', \n",
    "                                 'distance'])\n",
    "output.to_csv('../results/text_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>reference</th>\n",
       "      <th>generated</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gives directional light that is good for focus...</td>\n",
       "      <td>spaces</td>\n",
       "      <td>spaces</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>solid pine with crafted details and brushed su...</td>\n",
       "      <td>together to save space the surface is durable ...</td>\n",
       "      <td>together to save space easy to keep clean just...</td>\n",
       "      <td>and it in beautifully in you trellis free the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you can choose to assemble this coffee table a...</td>\n",
       "      <td>wipe clean with damp cloth</td>\n",
       "      <td>clean you can easily adapt</td>\n",
       "      <td>which open as the charge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the trellis makes it easy to decorate your wal...</td>\n",
       "      <td>enjoy the natural expression of the wood the f...</td>\n",
       "      <td>enjoy the natural expression of the wood the f...</td>\n",
       "      <td>pot to piece filling kitchen shopping hide she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sit comfortably thanks to the pocket sprin...</td>\n",
       "      <td>designed to be comfortable for you to lean aga...</td>\n",
       "      <td>designed to be comfortable for you to lean aga...</td>\n",
       "      <td>be and needs limited hide different the keep f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               start  \\\n",
       "0  gives directional light that is good for focus...   \n",
       "1  solid pine with crafted details and brushed su...   \n",
       "2  you can choose to assemble this coffee table a...   \n",
       "3  the trellis makes it easy to decorate your wal...   \n",
       "4  you sit comfortably thanks to the pocket sprin...   \n",
       "\n",
       "                                           reference  \\\n",
       "0                                             spaces   \n",
       "1  together to save space the surface is durable ...   \n",
       "2                         wipe clean with damp cloth   \n",
       "3  enjoy the natural expression of the wood the f...   \n",
       "4  designed to be comfortable for you to lean aga...   \n",
       "\n",
       "                                           generated  \\\n",
       "0                                             spaces   \n",
       "1  together to save space easy to keep clean just...   \n",
       "2                         clean you can easily adapt   \n",
       "3  enjoy the natural expression of the wood the f...   \n",
       "4  designed to be comfortable for you to lean aga...   \n",
       "\n",
       "                                              random  \n",
       "0                                                can  \n",
       "1  and it in beautifully in you trellis free the ...  \n",
       "2                           which open as the charge  \n",
       "3  pot to piece filling kitchen shopping hide she...  \n",
       "4  be and needs limited hide different the keep f...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
