{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randint\n",
    "from pickle import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import textstat\n",
    "import nltk\n",
    "from itertools import compress, cycle, islice\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# i'm not even using these anymore\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "random.seed(952)\n",
    "\n",
    "\n",
    "def clean_text(input):\n",
    "    # tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "    # remove non alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # remove tokens of length 1\n",
    "    tokens_len = [len(i) > 1 for i in tokens]\n",
    "    tokens_filter = list(compress(tokens, tokens_len))\n",
    "    tokens = tokens_filter\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# save tokens to file, one sequence per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "def detect_labels(path):\n",
    "    \"\"\"Detects labels in LOCAL file.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "\n",
    "    response = client.label_detection(image=image)\n",
    "    labels = response.label_annotations\n",
    "\n",
    "    # list of labels (ignoring uncertainty)\n",
    "    labels = [x.description for x in labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ikea catalogue\n",
    "ikea_cat = pd.read_csv('../results/ikea_2.csv')\n",
    "\n",
    "ikdes = ikea_cat.description\n",
    "\n",
    "tester = ['bookcase', 'shelf', 'hutch']\n",
    "\n",
    "# find which descriptions have any of the right words\n",
    "ikea_words = []\n",
    "for ii in range(0, len(ikdes)):\n",
    "    ct = clean_text(str((ikdes[ii])))\n",
    "    ikea_words.append(any(item in tester for item in ct))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearly visible chunks of chickpeas, green peas, carrots, bell peppers, corn and kale in a beautiful array of colors. An eating option with fewer calories and less fat. By eating more vegetables you reduce your carbon footprint. Includes no animal ingredients, therefore it's a good choice for vegetarians and vegans. Veggie balls should taste like vegetables. That’s why ours consist of nothing but vegetables. They’re a celebration of vegetables. The recipe does not contain ingredients with allergens such as gluten, soy, nut, egg or lactose so as many people as possible can enjoy them. A good source of fiber, protein, vitamins and nutrients, but low in cholesterol. Easily prepared from frozen in minutes. In the oven, in the microwave or on the stove. Easily portioned. Prepare the amount needed, leave the rest in the freezer. Can be combined in many different ways. For example, try these instead of meatballs in your favorite meatball dish. Just as tasty warm as they are cold, e.g. cut in half as a sandwich topping. Create a buffet with meatballs, veggie balls, chicken meatballs and salmon balls. One for every taste. \n",
      "Clearly visible bookcase of shelf , green hutch , bookcase , shelf hutch , bookcase and shelf in a beautiful hutch of bookcase . An eating shelf with fewer hutch and less fat . By eating more bookcase you reduce your shelf hutch . Includes no animal bookcase , therefore it 's a good shelf for hutch and bookcase . shelf hutch should taste like bookcase . That ’ s why shelf hutch of bookcase but shelf . They ’ re a hutch of bookcase . The shelf does not contain hutch with bookcase such as shelf , hutch , bookcase , shelf or lactose so as many hutch as possible can enjoy them . A good bookcase of shelf , hutch , bookcase and shelf , but low in hutch . Easily prepared from frozen in bookcase . In the shelf , in the hutch or on the bookcase . Easily portioned . Prepare the shelf needed , leave the hutch in the bookcase . Can be combined in many different shelf . For hutch , try these instead of bookcase in your favorite meatball shelf . hutch as tasty bookcase as they are cold , e.g . cut in shelf as a hutch topping . Create a bookcase with shelf , hutch bookcase , shelf hutch and bookcase shelf . One for every hutch .\n"
     ]
    }
   ],
   "source": [
    "def replace_nouns(text, replace):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    \n",
    "    tt = []\n",
    "    for ii in range(0, len(tagged)):\n",
    "        tt.append(tagged[ii][1][0] == 'N')\n",
    "        \n",
    "    replacements = list(islice(cycle(tester), sum(tt)))\n",
    "    \n",
    "    jj = 0\n",
    "    for ii in range(0, len(tagged)):\n",
    "        if tt[ii]:\n",
    "            tokenized[ii] = replacements[jj]\n",
    "            jj = jj + 1\n",
    "            \n",
    "    return ' '.join(tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
